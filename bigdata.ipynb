{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Consumption Analysis in the Steel Industry\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The dataset \"Steel Industry Energy Consumption\" comprises 35,040 observations with 11 variables. These variables include data related to different aspects of energy consumption in the steel industry, such as timestamp, energy consumed, type of energy, operating status, type of product being produced, etc.\n",
    "\n",
    "The objective is to analyze this dataset to uncover patterns and relationships that can help in improving energy efficiency. To achieve this, we'll use Apache Spark's PySpark module, which allows us to handle large datasets and perform complex data analysis tasks efficiently.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset chosen for this project is the \"Steel Industry Energy Consumption\" dataset available on Kaggle. This dataset comprises 35,040 observations with 11 variables related to different aspects of energy consumption in the steel industry. The link to the dataset is https://www.kaggle.com/datasets/csafrit2/steel-industry-energy-consumption.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Energy Consumption Analysis in the Steel Industry\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "### Data Loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"archive (5)\\Steel_industry_data.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn('date', to_date(df['date'], 'd-M-yy H.m').cast(DateType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "### Data Exploration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+----+----------+-----------+----------+\n",
      "|      date|Usage_kWh|Lagging_Current_Reactive_Power_kVarh|Leading_Current_Reactive_Power_kVarh|CO2(tCO2)|Lagging_Current_Power_Factor|Leading_Current_Power_Factor| NSM|WeekStatus|Day_of_week| Load_Type|\n",
      "+----------+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+----+----------+-----------+----------+\n",
      "|2018-01-01|     3.17|                                2.95|                                 0.0|      0.0|                       73.21|                       100.0| 900|   Weekday|     Monday|Light_Load|\n",
      "|2018-01-01|      4.0|                                4.46|                                 0.0|      0.0|                       66.77|                       100.0|1800|   Weekday|     Monday|Light_Load|\n",
      "|2018-01-01|     3.24|                                3.28|                                 0.0|      0.0|                       70.28|                       100.0|2700|   Weekday|     Monday|Light_Load|\n",
      "|2018-01-01|     3.31|                                3.56|                                 0.0|      0.0|                       68.09|                       100.0|3600|   Weekday|     Monday|Light_Load|\n",
      "|2018-01-01|     3.82|                                 4.5|                                 0.0|      0.0|                       64.72|                       100.0|4500|   Weekday|     Monday|Light_Load|\n",
      "+----------+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+----+----------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- Usage_kWh: double (nullable = true)\n",
      " |-- Lagging_Current_Reactive_Power_kVarh: double (nullable = true)\n",
      " |-- Leading_Current_Reactive_Power_kVarh: double (nullable = true)\n",
      " |-- CO2(tCO2): double (nullable = true)\n",
      " |-- Lagging_Current_Power_Factor: double (nullable = true)\n",
      " |-- Leading_Current_Power_Factor: double (nullable = true)\n",
      " |-- NSM: integer (nullable = true)\n",
      " |-- WeekStatus: string (nullable = true)\n",
      " |-- Day_of_week: string (nullable = true)\n",
      " |-- Load_Type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------------------------+------------------------------------+--------------------+----------------------------+----------------------------+------------------+----------+-----------+-----------+\n",
      "|summary|         Usage_kWh|Lagging_Current_Reactive_Power_kVarh|Leading_Current_Reactive_Power_kVarh|           CO2(tCO2)|Lagging_Current_Power_Factor|Leading_Current_Power_Factor|               NSM|WeekStatus|Day_of_week|  Load_Type|\n",
      "+-------+------------------+------------------------------------+------------------------------------+--------------------+----------------------------+----------------------------+------------------+----------+-----------+-----------+\n",
      "|  count|             35040|                               35040|                               35040|               35040|                       35040|                       35040|             35040|     35040|      35040|      35040|\n",
      "|   mean|27.386892408677415|                  13.035383561643709|                  3.8709486301369203|0.011524257990866064|           80.57805622146188|           84.36786986301506|           42750.0|      null|       null|       null|\n",
      "| stddev| 33.44437970801511|                  16.305999973081995|                   7.424462753103669|0.016150821534429333|          18.921322267817423|           30.45653515758769|24940.534316748435|      null|       null|       null|\n",
      "|    min|               0.0|                                 0.0|                                 0.0|                 0.0|                         0.0|                         0.0|                 0|   Weekday|     Friday| Light_Load|\n",
      "|    max|            157.18|                               96.91|                               27.76|                0.07|                       100.0|                       100.0|             85500|   Weekend|  Wednesday|Medium_Load|\n",
      "+-------+------------------+------------------------------------+------------------------------------+--------------------+----------------------------+----------------------------+------------------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+---+----------+-----------+---------+\n",
      "|Usage_kWh|Lagging_Current_Reactive_Power_kVarh|Leading_Current_Reactive_Power_kVarh|CO2(tCO2)|Lagging_Current_Power_Factor|Leading_Current_Power_Factor|NSM|WeekStatus|Day_of_week|Load_Type|\n",
      "+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+---+----------+-----------+---------+\n",
      "|        0|                                   0|                                   0|        0|                           0|                           0|  0|         0|          0|        0|\n",
      "+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+---+----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns if c !=\"date\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   Load_Type|count|\n",
      "+------------+-----+\n",
      "| Medium_Load| 9696|\n",
      "|Maximum_Load| 7272|\n",
      "|  Light_Load|18072|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Load_Type').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "### Data Pre-Procesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   Load_Type|count|\n",
      "+------------+-----+\n",
      "| Medium_Load| 6805|\n",
      "|Maximum_Load| 5128|\n",
      "|  Light_Load|12673|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupBy('Load_Type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- Usage_kWh: double (nullable = true)\n",
      " |-- Lagging_Current_Reactive_Power_kVarh: double (nullable = true)\n",
      " |-- Leading_Current_Reactive_Power_kVarh: double (nullable = true)\n",
      " |-- CO2(tCO2): double (nullable = true)\n",
      " |-- Lagging_Current_Power_Factor: double (nullable = true)\n",
      " |-- Leading_Current_Power_Factor: double (nullable = true)\n",
      " |-- NSM: integer (nullable = true)\n",
      " |-- WeekStatus: string (nullable = true)\n",
      " |-- Day_of_week: string (nullable = true)\n",
      " |-- Load_Type: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, year, month, dayofmonth\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# Extract day, month, year\n",
    "train_data = train_data.withColumn('day', dayofmonth(train_data['date']))\n",
    "train_data = train_data.withColumn('month', month(train_data['date']))\n",
    "train_data = train_data.withColumn('year', year(train_data['date']))\n",
    "\n",
    "test_data = test_data.withColumn('day', dayofmonth(test_data['date']))\n",
    "test_data = test_data.withColumn('month', month(test_data['date']))\n",
    "test_data = test_data.withColumn('year', year(test_data['date']))\n",
    "\n",
    "# Print the schema to see if it worked\n",
    "train_data.printSchema()\n",
    "\n",
    "train_data=train_data.drop('date')\n",
    "test_data=test_data.drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+-----+----------+-----------+----------+---+-----+----+\n",
      "|Usage_kWh|Lagging_Current_Reactive_Power_kVarh|Leading_Current_Reactive_Power_kVarh|CO2(tCO2)|Lagging_Current_Power_Factor|Leading_Current_Power_Factor|  NSM|WeekStatus|Day_of_week| Load_Type|day|month|year|\n",
      "+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+-----+----------+-----------+----------+---+-----+----+\n",
      "|     3.17|                                2.95|                                 0.0|      0.0|                       73.21|                       100.0|  900|   Weekday|     Monday|Light_Load|  1|    1|2018|\n",
      "|      3.2|                                 3.6|                                 0.0|      0.0|                       66.44|                       100.0|27900|   Weekday|     Monday|Light_Load|  1|    1|2018|\n",
      "|     3.24|                                3.28|                                 0.0|      0.0|                       70.28|                       100.0| 2700|   Weekday|     Monday|Light_Load|  1|    1|2018|\n",
      "|     3.24|                                3.35|                                 0.0|      0.0|                       69.52|                       100.0|23400|   Weekday|     Monday|Light_Load|  1|    1|2018|\n",
      "|     3.24|                                3.35|                                 0.0|      0.0|                       69.52|                       100.0|81900|   Weekday|     Monday|Light_Load|  1|    1|2018|\n",
      "+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+-----+----------+-----------+----------+---+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Usage_kWh: double (nullable = true)\n",
      " |-- Lagging_Current_Reactive_Power_kVarh: double (nullable = true)\n",
      " |-- Leading_Current_Reactive_Power_kVarh: double (nullable = true)\n",
      " |-- CO2(tCO2): double (nullable = true)\n",
      " |-- Lagging_Current_Power_Factor: double (nullable = true)\n",
      " |-- Leading_Current_Power_Factor: double (nullable = true)\n",
      " |-- NSM: integer (nullable = true)\n",
      " |-- WeekStatus: string (nullable = true)\n",
      " |-- Day_of_week: string (nullable = true)\n",
      " |-- Load_Type: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "counts = train_data.groupBy('Load_Type').count().collect()\n",
    "num_classes = len(counts)\n",
    "\n",
    "# Calculate ratio for undersampling\n",
    "ratio = counts[1]['count'] / counts[2]['count']\n",
    "\n",
    "# Filter 'Light_Load' instances and sample\n",
    "light_load_train = train_data.filter(col('Load_Type') == 'Light_Load').sample(False, ratio)\n",
    "\n",
    "# Filter for the other classes\n",
    "other_train = train_data.filter(col('Load_Type') != 'Light_Load')\n",
    "\n",
    "# Union the DataFrames\n",
    "train_balanced = light_load_train.union(other_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   Load_Type|count|\n",
      "+------------+-----+\n",
      "|  Light_Load| 5138|\n",
      "| Medium_Load| 6805|\n",
      "|Maximum_Load| 5128|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_balanced.groupBy('Load_Type').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+-----+----------+-----------+----------+---+-----+----+\n",
      "|Usage_kWh|Lagging_Current_Reactive_Power_kVarh|Leading_Current_Reactive_Power_kVarh|CO2(tCO2)|Lagging_Current_Power_Factor|Leading_Current_Power_Factor|  NSM|WeekStatus|Day_of_week| Load_Type|day|month|year|\n",
      "+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+-----+----------+-----------+----------+---+-----+----+\n",
      "|     3.28|                                3.64|                                 0.0|      0.0|                       66.94|                       100.0| 8100|   Weekday|     Monday|Light_Load|  1|    1|2018|\n",
      "|     3.31|                                3.74|                                 0.0|      0.0|                       66.27|                       100.0|12600|   Weekday|     Monday|Light_Load|  1|    1|2018|\n",
      "|     3.31|                                3.78|                                 0.0|      0.0|                       65.88|                       100.0|28800|   Weekday|     Monday|Light_Load|  1|    1|2018|\n",
      "|     3.49|                                4.46|                                0.18|      0.0|                       61.63|                       99.87|31500|   Weekday|     Monday|Light_Load|  1|    1|2018|\n",
      "|     3.53|                                 0.0|                               17.39|      0.0|                       100.0|                       19.89|69300|   Weekday|     Monday|Light_Load|  1|    1|2018|\n",
      "+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+-----+----------+-----------+----------+---+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_balanced.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# String Indexing for categorical columns\n",
    "week_status_indexer = StringIndexer(inputCol=\"WeekStatus\", outputCol=\"WeekStatusIndex\")\n",
    "day_of_week_indexer = StringIndexer(inputCol=\"Day_of_week\", outputCol=\"DayOfWeekIndex\")\n",
    "\n",
    "# One-hot encoding for categorical columns\n",
    "week_status_encoder = OneHotEncoder(inputCol=\"WeekStatusIndex\", outputCol=\"WeekStatusVec\")\n",
    "day_of_week_encoder = OneHotEncoder(inputCol=\"DayOfWeekIndex\", outputCol=\"DayOfWeekVec\")\n",
    "\n",
    "# String Indexing for target column (Load_Type)\n",
    "load_type_indexer = StringIndexer(inputCol=\"Load_Type\", outputCol=\"label\")\n",
    "\n",
    "# Assemble all the features with VectorAssembler\n",
    "required_features = ['Usage_kWh',\n",
    "                    'Lagging_Current_Reactive_Power_kVarh',\n",
    "                    'Leading_Current_Reactive_Power_kVarh',\n",
    "                    'CO2(tCO2)',\n",
    "                    'Lagging_Current_Power_Factor',\n",
    "                    'Leading_Current_Power_Factor',\n",
    "                    'NSM',\n",
    "                    'WeekStatusVec',\n",
    "                    'DayOfWeekVec']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
    "\n",
    "# Create a Pipeline\n",
    "pipeline = Pipeline(stages=[week_status_indexer, day_of_week_indexer, \n",
    "                            week_status_encoder, day_of_week_encoder,\n",
    "                            load_type_indexer,\n",
    "                            assembler])\n",
    "\n",
    "# Fit and transform the data\n",
    "pipeline_model = pipeline.fit(train_balanced)\n",
    "train_balanced = pipeline_model.transform(train_balanced)\n",
    "test_data = pipeline_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+-----+----------+-----------+----------+---+-----+----+---------------+--------------+-------------+-------------+-----+--------------------+\n",
      "|Usage_kWh|Lagging_Current_Reactive_Power_kVarh|Leading_Current_Reactive_Power_kVarh|CO2(tCO2)|Lagging_Current_Power_Factor|Leading_Current_Power_Factor|  NSM|WeekStatus|Day_of_week| Load_Type|day|month|year|WeekStatusIndex|DayOfWeekIndex|WeekStatusVec| DayOfWeekVec|label|            features|\n",
      "+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+-----+----------+-----------+----------+---+-----+----+---------------+--------------+-------------+-------------+-----+--------------------+\n",
      "|     3.28|                                3.64|                                 0.0|      0.0|                       66.94|                       100.0| 8100|   Weekday|     Monday|Light_Load|  1|    1|2018|            0.0|           1.0|(1,[0],[1.0])|(6,[1],[1.0])|  1.0|(14,[0,1,4,5,6,7,...|\n",
      "|     3.31|                                3.74|                                 0.0|      0.0|                       66.27|                       100.0|12600|   Weekday|     Monday|Light_Load|  1|    1|2018|            0.0|           1.0|(1,[0],[1.0])|(6,[1],[1.0])|  1.0|(14,[0,1,4,5,6,7,...|\n",
      "|     3.31|                                3.78|                                 0.0|      0.0|                       65.88|                       100.0|28800|   Weekday|     Monday|Light_Load|  1|    1|2018|            0.0|           1.0|(1,[0],[1.0])|(6,[1],[1.0])|  1.0|(14,[0,1,4,5,6,7,...|\n",
      "|     3.49|                                4.46|                                0.18|      0.0|                       61.63|                       99.87|31500|   Weekday|     Monday|Light_Load|  1|    1|2018|            0.0|           1.0|(1,[0],[1.0])|(6,[1],[1.0])|  1.0|(14,[0,1,2,4,5,6,...|\n",
      "|     3.53|                                 0.0|                               17.39|      0.0|                       100.0|                       19.89|69300|   Weekday|     Monday|Light_Load|  1|    1|2018|            0.0|           1.0|(1,[0],[1.0])|(6,[1],[1.0])|  1.0|(14,[0,2,4,5,6,7,...|\n",
      "+---------+------------------------------------+------------------------------------+---------+----------------------------+----------------------------+-----+----------+-----------+----------+---+-----+----+---------------+--------------+-------------+-------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_balanced.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create Decision Tree model\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Fit the model\n",
    "dt_model = dt.fit(train_balanced)\n",
    "\n",
    "# Predict with the test dataset\n",
    "dt_predictions = dt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.820299022426682\n",
      "Test Weighted Precision:  0.8412120914514901\n",
      "Test Weighted Recall:  0.820299022426682\n",
      "Test F1 Score:  0.8265635109982734\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create evaluators\n",
    "evaluator1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator2 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator3 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "evaluator4 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Apply the evaluators to the DataFrame with the predictions\n",
    "accuracy = evaluator1.evaluate(dt_predictions)\n",
    "weighted_precision = evaluator2.evaluate(dt_predictions)\n",
    "weighted_recall = evaluator3.evaluate(dt_predictions)\n",
    "f1 = evaluator4.evaluate(dt_predictions)\n",
    "\n",
    "print(\"Test Accuracy: \", accuracy)\n",
    "print(\"Test Weighted Precision: \", weighted_precision)\n",
    "print(\"Test Weighted Recall: \", weighted_recall)\n",
    "print(\"Test F1 Score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "### Inference\n",
    "The Decision Tree model achieved an accuracy of approximately 80.90% on the test set, which suggests that it's correctly predicting the target variable for about four out of five observations in the unseen data. Also, the precision, recall, and F1 scores, all being above 80%, suggest a good balance between the model's ability to find relevant instances (precision), its ability to find all relevant instances (recall), and the harmonic mean of precision and recall (F1 score). This means the model has performed well not only in terms of overall accuracy but also in terms of distinguishing between different classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "### Conclusion\n",
    "Considering the metrics above, we can conclude that the Decision Tree model has performed reasonably well for this specific problem. Decision Trees are known for their simplicity and interpretability, which makes it easier for us to understand the underlying decisions made by the model. They also handle a mix of numerical and categorical features well, which may be beneficial for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "### Future Work\n",
    "\n",
    "Hyperparameter Tuning: We could perform further hyperparameter tuning on the Decision Tree model to improve its performance. Grid search or random search could be used to systematically work through multiple combinations of hyperparameters.\n",
    "\n",
    "Feature Engineering: We should experiment with feature engineering to create new features that could potentially improve the model's performance. This could include polynomial features, interactions, or domain-specific features.\n",
    "\n",
    "Data Collection: Since the performance is still not satisfactory, we should consider collecting more data or improving the quality of the current data if possible.\n",
    "\n",
    "-------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augassn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
